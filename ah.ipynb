{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float, Date, text\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load each CSV into its own dataframe ---\n",
    "df_dataset = pd.read_csv(\"/Users/Vengeance/Documents/GitHub/earthquake-analysis-japan/JAPAN_DATASET_cleaned.csv\")\n",
    "df_emsc = pd.read_csv(\"/Users/Vengeance/Documents/GitHub/earthquake-analysis-japan/JAPAN_EMSC_cleaned.csv\")\n",
    "df_geofon = pd.read_csv(\"/Users/Vengeance/Documents/GitHub/earthquake-analysis-japan/JAPAN_GEOFON_cleaned.csv\")\n",
    "df_usgs = pd.read_csv(\"/Users/Vengeance/Documents/GitHub/earthquake-analysis-japan/JAPAN_USGS_cleaned.csv\")\n",
    "\n",
    "# --- Clean and prepare each df ---\n",
    "def clean_df(df, source_name):\n",
    "    if 'place' in df.columns and 'region' in df.columns:\n",
    "        df = df.drop(columns=['place'])\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        df['coordination'] = df.apply(lambda x: f\"[{x.latitude}, {x.longitude}]\", axis=1)\n",
    "    rename_map = {}\n",
    "    if 'mag' in df.columns:\n",
    "        rename_map['mag'] = 'magnitude'\n",
    "    if 'place' in df.columns and 'region' not in df.columns:\n",
    "        rename_map['place'] = 'region'\n",
    "    df = df.rename(columns=rename_map)\n",
    "    df['source'] = source_name\n",
    "    columns_needed = ['time', 'coordination', 'depth', 'magnitude', 'region', 'source']\n",
    "    df = df[[c for c in columns_needed if c in df.columns]]\n",
    "    if 'time' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['time'], errors='coerce').dt.date\n",
    "    if 'depth' in df.columns:\n",
    "        df['depth'] = pd.to_numeric(df['depth'], errors='coerce')\n",
    "    if 'magnitude' in df.columns:\n",
    "        df['magnitude'] = pd.to_numeric(df['magnitude'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "df_dataset = clean_df(df_dataset, \"Dataset\")\n",
    "df_emsc = clean_df(df_emsc, \"EMSC\")\n",
    "df_geofon = clean_df(df_geofon, \"GEOFON\")\n",
    "df_usgs = clean_df(df_usgs, \"USGS\")\n",
    "\n",
    "# --- Join all datasets into one final DataFrame using .join() ---\n",
    "df_dataset = df_dataset.reset_index(drop=True)\n",
    "df_emsc = df_emsc.reset_index(drop=True)\n",
    "df_geofon = df_geofon.reset_index(drop=True)\n",
    "df_usgs = df_usgs.reset_index(drop=True)\n",
    "\n",
    "df_final = df_dataset.join(df_emsc, rsuffix='_emsc', how='outer')\n",
    "df_final = df_final.join(df_geofon, rsuffix='_geofon', how='outer')\n",
    "df_final = df_final.join(df_usgs, rsuffix='_usgs', how='outer')\n",
    "\n",
    "# Keep only the schema columns for SQL\n",
    "final_columns = ['time', 'coordination', 'depth', 'magnitude', 'region', 'source']\n",
    "df_final = df_final[final_columns].dropna(subset=['time', 'magnitude', 'region']).drop_duplicates()\n",
    "\n",
    "# --- SQL setup ---\n",
    "engine = create_engine(\"mysql+pymysql://root:nimaaslrousta717@localhost:3306/earthquakes_db\")\n",
    "connection = engine.connect()\n",
    "metadata = MetaData()\n",
    "\n",
    "earthquakes = Table(\n",
    "    'Earthquakes',\n",
    "    metadata,\n",
    "    Column('id', Integer, primary_key=True),\n",
    "    Column('time', Date),\n",
    "    Column('coordination', String(50)),\n",
    "    Column('depth', Float),\n",
    "    Column('magnitude', Float),\n",
    "    Column('region', String(100)),\n",
    "    Column('source', String(50))\n",
    ")\n",
    "\n",
    "connection.execute(text(\"DROP TABLE IF EXISTS Earthquakes;\"))\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# --- Insert joined dataframe into SQL ---\n",
    "df_final.to_sql(\n",
    "    name='Earthquakes',\n",
    "    con=engine,\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    "    chunksize=1000\n",
    ")\n",
    "\n",
    "count = connection.execute(text(\"SELECT COUNT(*) FROM Earthquakes;\")).scalar()\n",
    "print(f\"Number of rows inserted: {count}\")\n",
    "\n",
    "# --- Queries ---\n",
    "queries = {\n",
    "    \"total_earthquakes\": \"\"\"\n",
    "        SELECT region, EXTRACT(MONTH FROM time) AS month, COUNT(*) AS total_earthquakes\n",
    "        FROM Earthquakes\n",
    "        GROUP BY region, month;\n",
    "    \"\"\",\n",
    "    \"avg_magnitude\": \"\"\"\n",
    "        SELECT region, source, AVG(magnitude) AS avg_magnitude\n",
    "        FROM Earthquakes\n",
    "        GROUP BY region, source;\n",
    "    \"\"\",\n",
    "    \"top_earthquakes\": \"\"\"\n",
    "        SELECT *\n",
    "        FROM Earthquakes\n",
    "        ORDER BY magnitude DESC, time DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\",\n",
    "    \"depth_range\": \"\"\"\n",
    "        SELECT region, MAX(depth) AS max_depth, MIN(depth) AS min_depth\n",
    "        FROM Earthquakes\n",
    "        GROUP BY region;\n",
    "    \"\"\",\n",
    "    \"delete_invalid\": \"\"\"\n",
    "        DELETE FROM Earthquakes\n",
    "        WHERE magnitude < 0\n",
    "           OR magnitude > 10\n",
    "           OR depth < 0;\n",
    "    \"\"\",\n",
    "    \"update_null_magnitude\": \"\"\"\n",
    "        UPDATE Earthquakes\n",
    "        SET magnitude = 0\n",
    "        WHERE magnitude IS NULL;\n",
    "    \"\"\",\n",
    "    \"update_null_depth\": \"\"\"\n",
    "        UPDATE Earthquakes\n",
    "        SET depth = 0\n",
    "        WHERE depth IS NULL;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for q in queries.values():\n",
    "    connection.execute(text(q))\n",
    "\n",
    "result = connection.execute(\n",
    "    text(\"SELECT id, time, coordination, depth, magnitude, region, source FROM Earthquakes LIMIT 5;\")\n",
    ").fetchall()\n",
    "for row in result:\n",
    "    print(row)\n",
    "\n",
    "output_path = \"Earthquakes_export.csv\"\n",
    "df_export = pd.read_sql(\"SELECT * FROM Earthquakes\", con=engine)\n",
    "df_export.to_csv(output_path, index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
